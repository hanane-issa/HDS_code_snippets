#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------
# Model: ResNet34
#-------------------------------------------------------------------------------

#-----residual unit function combining identity and convolutional blocks
def res_unit(inputs, n_filters, start_block=False):
  '''
  Using Keras functional API.
  The residual unit which will be repeated across the resnet34 model.
  It is composed of two convoluational layers with the same filter number and size, 
  as well as an identity shortcut that adds the input of the 1st layer to the output of the 2nd one.
  When the input and output sizes don't match (i.e. when going from one block to the other with increasing filter size), 
  a projection shortcut is applied, i.e. a 1x1 convolution layer with a stride of 2 to reduce dimensionality.
  '''  

  merge_in= inputs
	#projection shortcut for dimensionality reduction (match dimensions of input and output)
  if inputs.shape[-1] != n_filters:
    merge_in = Conv2D(n_filters, (1,1), strides=(2,2), padding='same', activation='relu')(inputs) #its not working
    merge_in = BatchNormalization()(merge_in)

  #conv1 
  #downsample if first convolutional layer in block
  if start_block==True:
    conv1 = Conv2D(n_filters, (3,3), padding='same', strides=(2,2), activation='relu')(inputs)
  else:
    conv1 = Conv2D(n_filters, (3,3), padding='same', strides=(1,1), activation='relu')(inputs)
  conv1 = BatchNormalization()(conv1)
  
  
	# conv2
  conv2 = Conv2D(n_filters, (3,3), padding='same', strides=(1,1), activation='relu')(conv1)
  conv2= BatchNormalization()(conv2)

	# identity shortcut connection
  out1 = Add()([conv2, merge_in])
	# activation function
  out1 = Activation('relu')(out1)
  return out1
  
  #-------------------------------------------------------------------------------
  #-----Function using previous res unit function and building resnet34 architecture
def resnet34(input_shape=(256, 256, 1)):

  random.seed(42)
  np.random.seed(42)
  tf.random.set_seed(42)

  inputs=Input(input_shape)

  #----- LAYER 0
  x =Conv2D(64, (7, 7), strides =2, padding="same")(inputs)
  x =BatchNormalization()(x)
  x = Activation("relu")(x)
  x = MaxPool2D((3, 3),strides=2)(x)

  #----- LAYER 1
  n_filters =64
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)

  #----- LAYER 2
  n_filters =128
  x=res_unit(x,n_filters, start_block=True)
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)


  #----- LAYER 3
  n_filters =256
  x=res_unit(x,n_filters, start_block=True)
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)

  #----- LAYER 4
  n_filters =512
  x=res_unit(x,n_filters, start_block=True)
  x=res_unit(x,n_filters)
  x=res_unit(x,n_filters)


  #----- LAYER END
  x =GlobalAveragePooling2D()(x)
  x=Flatten()(x)
  pred = Dense(2, activation='softmax')(x) 
  print(pred.shape)

  model = Model(inputs=inputs, outputs=pred, name="ResNet-34")

  return model
